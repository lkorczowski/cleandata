{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing a Gensim model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate how to use [`pyLDAvis`](https://github.com/bmabey/pyLDAvis)'s gensim [helper funtions](https://pyldavis.readthedocs.org/en/latest/modules/API.html#module-pyLDAvis.gensim) we will create a model from the [20 Newsgroup corpus](http://qwone.com/~jason/20Newsgroups/). Minimal preprocessing is done and so the model is not the best. However, the goal of this notebook is to demonstrate the helper functions.\n",
    "\n",
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "~/git/cleandata/notebooks/pyLDAvis/data ~/git/cleandata/notebooks/pyLDAvis\nThe data has already been downloaded...\nLets take a look at the groups...\nalt.atheism\ncomp.graphics\ncomp.os.ms-windows.misc\ncomp.sys.ibm.pc.hardware\ncomp.sys.mac.hardware\ncomp.windows.x\nmisc.forsale\nrec.autos\nrec.motorcycles\nrec.sport.baseball\nrec.sport.hockey\nsci.crypt\nsci.electronics\nsci.med\nsci.space\nsoc.religion.christian\ntalk.politics.guns\ntalk.politics.mideast\ntalk.politics.misc\ntalk.religion.misc\n~/git/cleandata/notebooks/pyLDAvis\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir -p data\n",
    "pushd data\n",
    "if [ -d \"20news-bydate-train\" ]\n",
    "then\n",
    "  echo \"The data has already been downloaded...\"\n",
    "else\n",
    "  wget http://qwone.com/%7Ejason/20Newsgroups/20news-bydate.tar.gz\n",
    "  tar xfv 20news-bydate.tar.gz\n",
    "  rm 20news-bydate.tar.gz\n",
    "fi\n",
    "echo \"Lets take a look at the groups...\"\n",
    "ls 20news-bydate-train/\n",
    "popd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each group dir has a set of files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-2b580f01a234>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ls -lah data/20news-bydate-train/sci.space | tail  -n 5\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ],
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-2b580f01a234>, line 1)",
     "output_type": "error"
    }
   ],
   "source": [
    "ls -lah data/20news-bydate-train/sci.space | tail  -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a peak at one email:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "bash: line 1: !head: command not found\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-59e487f577be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'!head data/20news-bydate-train/sci.space/61422 -n 20\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/cleandata-env/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2360\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2363\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cleandata-env/lib/python3.8/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-110>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cleandata-env/lib/python3.8/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cleandata-env/lib/python3.8/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'!head data/20news-bydate-train/sci.space/61422 -n 20\\n\\n'' returned non-zero exit status 127."
     ],
     "ename": "CalledProcessError",
     "evalue": "Command 'b'!head data/20news-bydate-train/sci.space/61422 -n 20\\n\\n'' returned non-zero exit status 127.",
     "output_type": "error"
    }
   ],
   "source": [
    "!head data/20news-bydate-train/sci.space/61422 -n 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and tokenizing the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "config loaded\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from glob import glob\n",
    "import re\n",
    "import string\n",
    "import funcy as fp\n",
    "from gensim import models\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "print(\"config loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                                                           doc  \\\ngroup                 id                                                         \ntalk.politics.mideast 75895  [From: hm@cs.brown.edu (Harry Mamaysky)\\n, Sub...   \n                      76248  [From: waldo@cybernet.cse.fau.edu (Todd J. Dic...   \n                      76277  [From: C.L.Gannon@newcastle.ac.uk (Space Cadet...   \n                      76045  [From: shaig@Think.COM (Shai Guday)\\n, Subject...   \n                      76283  [From: koc@rize.ECE.ORST.EDU (Cetin Kaya Koc)\\...   \n\n                                                                        tokens  \ngroup                 id                                                        \ntalk.politics.mideast 75895  [from, #email, harry, mamaysky, subject, heil,...  \n                      76248  [from, #email, todd, dicker, subject, israel's...  \n                      76277  [from, #email, space, cadet, subject, exact, m...  \n                      76045  [from, #email, shai, guday, subject, basil, op...  \n                      76283  [from, #email, cetin, kaya, koc, subject, seve...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>doc</th>\n      <th>tokens</th>\n    </tr>\n    <tr>\n      <th>group</th>\n      <th>id</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">talk.politics.mideast</th>\n      <th>75895</th>\n      <td>[From: hm@cs.brown.edu (Harry Mamaysky)\\n, Sub...</td>\n      <td>[from, #email, harry, mamaysky, subject, heil,...</td>\n    </tr>\n    <tr>\n      <th>76248</th>\n      <td>[From: waldo@cybernet.cse.fau.edu (Todd J. Dic...</td>\n      <td>[from, #email, todd, dicker, subject, israel's...</td>\n    </tr>\n    <tr>\n      <th>76277</th>\n      <td>[From: C.L.Gannon@newcastle.ac.uk (Space Cadet...</td>\n      <td>[from, #email, space, cadet, subject, exact, m...</td>\n    </tr>\n    <tr>\n      <th>76045</th>\n      <td>[From: shaig@Think.COM (Shai Guday)\\n, Subject...</td>\n      <td>[from, #email, shai, guday, subject, basil, op...</td>\n    </tr>\n    <tr>\n      <th>76283</th>\n      <td>[From: koc@rize.ECE.ORST.EDU (Cetin Kaya Koc)\\...</td>\n      <td>[from, #email, cetin, kaya, koc, subject, seve...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 2
    }
   ],
   "source": [
    "# quick and dirty....\n",
    "EMAIL_REGEX = re.compile(r\"[a-z0-9\\.\\+_-]+@[a-z0-9\\._-]+\\.[a-z]*\")\n",
    "FILTER_REGEX = re.compile(r\"[^a-z '#]\")\n",
    "TOKEN_MAPPINGS = [(EMAIL_REGEX, \"#email\"), (FILTER_REGEX, ' ')]\n",
    "\n",
    "def tokenize_line(line):\n",
    "    res = line.lower()\n",
    "    for regexp, replacement in TOKEN_MAPPINGS:\n",
    "        res = regexp.sub(replacement, res)\n",
    "    return res.split()\n",
    "    \n",
    "def tokenize(lines, token_size_filter=2):\n",
    "    tokens = fp.mapcat(tokenize_line, lines)\n",
    "    return [t for t in tokens if len(t) > token_size_filter]\n",
    "    \n",
    "\n",
    "def load_doc(filename):\n",
    "    group, doc_id = filename.split('/')[-2:]\n",
    "    with open(filename, errors='ignore') as f:\n",
    "        doc = f.readlines()\n",
    "    return {'group': group,\n",
    "            'doc': doc,\n",
    "            'tokens': tokenize(doc),\n",
    "            'id': doc_id}\n",
    "\n",
    "\n",
    "docs = pd.DataFrame(list(map(load_doc, glob('data/20news-bydate-train/*/*')))).set_index(['group','id'])\n",
    "docs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the dictionary, and bag of words corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def nltk_stopwords():\n",
    "    nltk.download('stopwords')\n",
    "    return set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def prep_corpus(docs, additional_stopwords=set(), no_below=5, no_above=0.5):\n",
    "  print('Building dictionary...')\n",
    "  dictionary = Dictionary(docs)\n",
    "  stopwords = nltk_stopwords().union(additional_stopwords)\n",
    "  stopword_ids = map(dictionary.token2id.get, stopwords)\n",
    "  dictionary.filter_tokens(stopword_ids)\n",
    "  dictionary.compactify()\n",
    "  dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=None)\n",
    "  dictionary.compactify()\n",
    "\n",
    "  print('Building corpus...')\n",
    "  corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "  return dictionary, corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Building dictionary...\n",
      "Building corpus...\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/louis/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "dictionary, corpus = prep_corpus(docs['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "MmCorpus.serialize('newsgroups.mm', corpus)\n",
    "dictionary.save('newsgroups.dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=50, passes=10)\n",
    "                                      \n",
    "lda.save('newsgroups_50_lda.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the model with pyLDAvis\n",
    "\n",
    "Okay, the moment we have all been waiting for is finally here!  You'll notice in the visualization that we have a few junk topics that would probably disappear after better preprocessing of the corpus. This is left as an exercises to the reader. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim as gensimvis\n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data = gensimvis.prepare(lda, corpus, dictionary)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the HDP model\n",
    "\n",
    "We can both visualize LDA models as well as gensim HDP models with pyLDAvis.\n",
    "\n",
    "The difference between HDP and LDA is that HDP is a non-parametric method. Which means that we don't need to specify the number of topics. HDP will fit as many topics as it can and find the optimal number of topics by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# The optional parameter T here indicates that HDP should find no more than 50 topics\n",
    "# if there exists any.\n",
    "hdp = models.hdpmodel.HdpModel(corpus, dictionary, T=50)\n",
    "                                      \n",
    "hdp.save('newsgroups_hdp.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the HDP model with pyLDAvis\n",
    "\n",
    "As for the LDA model, in order to prepare the visualization you only need to pass it your model, the corpus, and the associated dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data = gensimvis.prepare(hdp, corpus, dictionary)\n",
    "pyLDAvis.display(vis_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "name": "Gensim Newsgroup.ipynb",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}